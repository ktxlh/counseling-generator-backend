{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Generator, Predictor\n",
    "generator = Generator(\"/home/ubuntu/models/generator/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AF', 'SUP', 'PR', 'QUC', 'RF', 'QUO', 'INT', 'GR']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['You are a very strong person',\n",
       " \"That's understandable\",\n",
       " 'You can do it',\n",
       " 'Do you think that they will be able to help?',\n",
       " 'That is a lot of pressure',\n",
       " 'Why do you think they are forcing you?',\n",
       " 'Hey',\n",
       " 'Oh no']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data = {\n",
    "    \"utterance\": [\"<|client|>I don't want to prepare for the final exams anymore, but my parents forced me to.\",],\n",
    "    \"generator_input_ids\": [[],],\n",
    "    \"is_listener\": [False,],\n",
    "})\n",
    "code_scores = [(code, 0.5) for code in Predictor.CODES]\n",
    "print(Predictor.CODES)\n",
    "gens = generator.predict(df, code_scores)\n",
    "gens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute seq scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "input_ids = [y for x in df.iloc[:][\"generator_input_ids\"].tolist() for y in x] + [-1,]  # placeholder for a code\n",
    "input_ids = torch.tensor(input_ids).view(1, -1)  # torch.LongTensor of shape (batch_size, sequence_length)\n",
    "input_ids = input_ids.repeat(len(code_scores), 1)\n",
    "input_ids[:, -1] = torch.LongTensor([generator.CODE_TOKEN_IDS[code] for (code, _) in code_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR    80.8 You can always talk with your family about it\n",
      "RF    78.4 It's okay you can do it\n",
      "QUC   73.6 Do you feel like your parents are being too hard on you?\n",
      "AF    59.7 That's a good decision\n",
      "SUP   51.5 That's understandable\n",
      "INT   22.5 Hey\n",
      "GR    22.5 Ohh\n",
      "QUO    0.0 Why did they force you?\n"
     ]
    }
   ],
   "source": [
    "decode = generator.tokenizer.decode\n",
    "\n",
    "generator.tokenizer.eos_token_id = generator.tokenizer.pad_token_id\n",
    "outputs = generator.model.generate(\n",
    "    input_ids.to(device), \n",
    "    do_sample=True, \n",
    "    max_length=input_ids.shape[1] + Generator.MAX_NEW_LEN,\n",
    "    top_p=0.95, \n",
    "    top_k=50,\n",
    "\n",
    "    length_penalty=0.9,\n",
    "    temperature=0.3,\n",
    "    repetition_penalty=1.2,\n",
    "    no_repeat_ngram_size=2,\n",
    "    forced_eos_token_id=generator.tokenizer.eos_token_id,\n",
    "\n",
    "    bad_word_ids=generator.bad_words_ids,\n",
    "    num_return_sequences=3,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    ")\n",
    "\n",
    "# Get scores of generated tokens\n",
    "ids = outputs.sequences[:, input_ids.shape[1]:]\n",
    "aranged = torch.arange(ids.shape[1]).repeat(ids.shape[0], 1)\n",
    "scores = torch.stack(outputs.scores)\n",
    "token_scores = scores[aranged, 0, ids]\n",
    "\n",
    "# Average scores of non-special tokens\n",
    "n_tokens = (token_scores >= 0).sum(axis=1)\n",
    "n_tokens[n_tokens == 0] = 1  # avoid nan\n",
    "token_scores[token_scores < 0] = 0.0\n",
    "seq_scores = token_scores.sum(axis=1) / n_tokens\n",
    "\n",
    "decoded = [decode(x, skip_special_tokens=True) for x in ids]\n",
    "# score_seq_tuples = [(score, d) for score, d in zip(seq_scores, decoded)]\n",
    "# score_seq_tuples.sort(key=lambda x: -x[0])\n",
    "# score_seq_tuples\n",
    "\n",
    "per_code_score_seq = []\n",
    "inc = 3\n",
    "for i in range(0, len(seq_scores), inc):\n",
    "    code = Predictor.CODES[i // inc]\n",
    "    idx = seq_scores[i:i+inc].argmax()\n",
    "    # for score, seq in zip(seq_scores, outputs.sequences):\n",
    "    # print(\"{:4.1f} {}\".format(seq_scores[i+idx], decoded[i+idx]))\n",
    "    per_code_score_seq.append((code, seq_scores[i+idx], decoded[i+idx]))\n",
    "\n",
    "per_code_score_seq.sort(key=lambda x: -x[1])\n",
    "for t in per_code_score_seq:\n",
    "    print(\"{:5} {:4.1f} {}\".format(*t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse scoring -- can't do it without actually training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 21]), torch.Size([24, 17]))"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape, ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 50267])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ro = generator.model.generate(\n",
    "    ids.to(device),\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    ")\n",
    "r_scores = torch.stack(ro.scores).mean(axis=0)\n",
    "r_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.9348, 22.4214, 15.3724, 18.8695, 15.9672, 19.5457, 15.2774, 18.4659,\n",
       "        17.2179, 14.7451, 13.8014, 16.4618, 23.8505, 16.8800, 17.2270, 11.7806,\n",
       "        13.1184, 15.5898, 19.5457, 27.0341, 11.9337], device='cuda:0')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_seq_score = r_scores[torch.arange(r_scores.shape[0]).reshape(-1, 1), input_ids.repeat(3, 1)].mean(axis=0)\n",
    "r_seq_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.0 Hey\n",
      "23.9 It sounds like you are going through a lot right now\n",
      "22.4 That's a good start\n",
      "19.5 Oh I'm sorry\n",
      "19.5 Hey\n",
      "18.9 That's understandable\n",
      "18.5 you shouldnt worry about that\n",
      "17.2 You are in a tough situation\n",
      "17.2 You should go back and study\n",
      "16.9 That's a lot of pressure on you\n",
      "16.5 Are you going to be able to get a job?\n",
      "16.0 Oh I understand\n",
      "15.6 Why did they force you?\n",
      "15.4 It's a good idea\n",
      "15.3 You can do it!\n",
      "14.7 So you are afraid of your future?\n",
      "13.8 Have you tried talking with your teachers about this?\n",
      "13.1 Why do you feel like they are forcing you?\n",
      "11.9 Hey\n",
      "11.8 How long have you been in this relationship?\n",
      "10.9 You're doing great\n"
     ]
    }
   ],
   "source": [
    "r_rs = [(r, decode(s, skip_special_tokens=True)) for r, s in zip(r_seq_score, ids)]\n",
    "r_rs.sort(key=lambda x: -x[0])\n",
    "for r, s in r_rs:\n",
    "    print(\"{:4.1f} {}\".format(r, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
